<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Is Batman more like the Joker or more like Robin?</title>
  
  <meta property="description" itemprop="description" content="Spend enough time in academia and you might start to think that this is an excellent research question. In this post&#10;I try to give an answer, by calculating the website similarities between the Wikipedia pages of Batman, the Joker and&#10;Robin. Some web scraping, data wrangling and NLP with Python lies ahead!"/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2020-06-14"/>
  <meta property="article:created" itemprop="dateCreated" content="2020-06-14"/>
  <meta name="article:author" content="Stefan Stein"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Is Batman more like the Joker or more like Robin?"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Spend enough time in academia and you might start to think that this is an excellent research question. In this post&#10;I try to give an answer, by calculating the website similarities between the Wikipedia pages of Batman, the Joker and&#10;Robin. Some web scraping, data wrangling and NLP with Python lies ahead!"/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Is Batman more like the Joker or more like Robin?"/>
  <meta property="twitter:description" content="Spend enough time in academia and you might start to think that this is an excellent research question. In this post&#10;I try to give an answer, by calculating the website similarities between the Wikipedia pages of Batman, the Joker and&#10;Robin. Some web scraping, data wrangling and NLP with Python lies ahead!"/>
  <meta property="twitter:site" content="@SteinsZeit"/>
  <meta property="twitter:creator" content="@SteinsZeit"/>
  
  <!--/radix_placeholder_meta_tags-->
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","twitter","categories","output","repository_url","preview"]}},"value":[{"type":"character","attributes":{},"value":["Is Batman more like the Joker or more like Robin?"]},{"type":"character","attributes":{},"value":["Spend enough time in academia and you might start to think that this is an excellent research question. In this post\nI try to give an answer, by calculating the website similarities between the Wikipedia pages of Batman, the Joker and\nRobin. Some web scraping, data wrangling and NLP with Python lies ahead!\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Stefan Stein"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":[]}},"value":[]}]}]},{"type":"character","attributes":{},"value":["06-14-2020"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["site","creator"]}},"value":[{"type":"character","attributes":{},"value":["@SteinsZeit"]},{"type":"character","attributes":{},"value":["@SteinsZeit"]}]},{"type":"character","attributes":{},"value":["webcrawling","Python","NLP","tf-idf"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["https://github.com/stefan-stein/website_similarites"]},{"type":"character","attributes":{},"value":["the-lego-batman-movie-the-joker-robin-batman-wallpaper-preview.jpg"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["batman_selector_1.png","batman_selector_2.png","the-lego-batman-movie-the-joker-robin-batman-wallpaper-preview.jpg","websitesimilarities_files/bowser-1.9.3/bowser.min.js","websitesimilarities_files/distill-2.2.21/template.v2.js","websitesimilarities_files/figure-html5/heatmap-1.png","websitesimilarities_files/figure-html5/unnamed-chunk-15-1.png","websitesimilarities_files/figure-html5/unnamed-chunk-16-1.png","websitesimilarities_files/figure-html5/unnamed-chunk-18-1.png","websitesimilarities_files/figure-html5/unnamed-chunk-19-1.png","websitesimilarities_files/figure-html5/unnamed-chunk-2-1.png","websitesimilarities_files/figure-html5/unnamed-chunk-22-1.png","websitesimilarities_files/figure-html5/unnamed-chunk-26-1.png","websitesimilarities_files/figure-html5/unnamed-chunk-27-1.png","websitesimilarities_files/figure-html5/unnamed-chunk-29-1.png","websitesimilarities_files/figure-html5/unnamed-chunk-31-1.png","websitesimilarities_files/figure-html5/unnamed-chunk-32-1.png","websitesimilarities_files/jquery-1.11.3/jquery.min.js","websitesimilarities_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="websitesimilarities_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="websitesimilarities_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="websitesimilarities_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="websitesimilarities_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Is Batman more like the Joker or more like Robin?","description":"Spend enough time in academia and you might start to think that this is an excellent research question. In this post\nI try to give an answer, by calculating the website similarities between the Wikipedia pages of Batman, the Joker and\nRobin. Some web scraping, data wrangling and NLP with Python lies ahead!","authors":[{"author":"Stefan Stein","authorURL":{},"affiliation":"&nbsp;","affiliationURL":"#"}],"publishedDate":"2020-06-14T00:00:00.000+02:00","citationText":"Stein, 2020"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Is Batman more like the Joker or more like Robin?</h1>
<p>Spend enough time in academia and you might start to think that this is an excellent research question. In this post I try to give an answer, by calculating the website similarities between the Wikipedia pages of Batman, the Joker and Robin. Some web scraping, data wrangling and NLP with Python lies ahead!</p>
</div>

<div class="d-byline">
  Stefan Stein true 
  
<br/>06-14-2020
</div>

<div class="d-article">
<div class="layout-chunk" data-layout="l-body">

</div>
<p><img src="the-lego-batman-movie-the-joker-robin-batman-wallpaper-preview.jpg" /></p>
<p>I’m pretty sure nobody really needs an introduction to who Batman is, as he is probably one of the most iconic superheroes. All too iconic are also his best buddy Robin and his nemesis, the Joker. It’s save to say that Batman would not be Batman without both of them in his life. So, having two such influential people in his life, it naturally (well…) begs the very important question: Whom is Batman more alike? Robin or the Joker?</p>
<p>Having spent some years as a PhD student, I know what to do in a situation like this: Having found a pressing question such as the above, we are going to conduct some rigorous research to get to the bottom of this! You can find the full code as a Jupyter notebook in the associated <a href="https://github.com/stefan-stein/website_similarites">GitHub repository</a>.</p>
<p>As in any good research project, we need to make sure our research question is actually well-defined. That is, first of all we need to define what it means for Batman to be <em>more like</em> the Joker or Robin. What are Batman and the other two “like”? Well, let’s go full mathematician on this problem and provide a formal definition:</p>
<p><strong>Definition:</strong> <em>Batman, the Joker and Robin are whatever their Wikipedia pages say they are.</em></p>
<p>At this point you might get a hunch that this project might not be entirely suitable for peer reviewed publication. But even though the project at hand is more of a fun little pastime, the techniques we are going to use are very useful for more serious research endeavors.</p>
<p>The definition above gives us an idea about what is to be done next: First, we want to obtain a copy of the Wikipedia pages for Batman, Robin and the Joker. Then, we somehow have to define a measure of similarity between pages. Mathematically speaking, we need to find a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a> <span class="math inline">\(d\)</span> on the space of websites <span class="math inline">\(\mathcal{W}\)</span>, that allows us to measure how close two websites are to one another. Mathematically, the space of websites is just some very large, finite set without any inherent structure. Hence, it will be difficult to define any meaningful metric on that space itself. Instead, we will find a map <span class="math inline">\(\varphi: \mathcal{W} \rightarrow \mathbb{R}^n\)</span>, for some potentially large <span class="math inline">\(n\)</span>, which, given a website, returns a numerical representation of that website as some high-dimensional vector, that captures “the essence” (I am deliberately vague about this here) of the website. We then choose an appropriate metric <span class="math inline">\(d\)</span> on <span class="math inline">\(\mathbb{R}^n \times \mathbb{R}^n\)</span> and measure the distance between two websites <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> as <span class="math inline">\(d(\varphi(A), \varphi(B))\)</span>. What do we gain from this? Well, there are plenty of metrics and associated similarity measures on <span class="math inline">\(\mathbb{R}^{n}\)</span> to choose from with well understood, desirable properties.</p>
<p>Wherever there is something to be gained from a simplification like this, there is a price to be paid, too. In our case it is that the map <span class="math inline">\(d(\varphi(.), \varphi(.)): \mathcal{W} \times \mathcal{W} \rightarrow \mathbb{R}\)</span> actually no longer needs to be a metric, as <span class="math inline">\(\varphi\)</span> need not be an embedding; i.e. two different websites could have the same numerical representation as an <span class="math inline">\(n\)</span>-dimensional vector under <span class="math inline">\(\varphi\)</span>. Especially for the embeddings we are going to use, which are based on word frequencies, this will be the case. It seems a reasonable price to pay, though, as we might heuristically assume that websites that use exactly the same words at the same frequency are “essentially” the same, too - although this can be a huge fallacy in more serious applications. For the sake of our small project at hand, we will go with it.</p>
<p>Let’s make a list of the things we need to do.</p>
<p><strong>To-Do List:</strong></p>
<ol type="1">
<li>Download the HTML of the Wikipedia pages for Batman, Robin and the Joker.</li>
<li>Pre-process the HTML: Get rid of all the tags, sidebars etc. so that only the actual page content remains.</li>
<li>For each website, find some representation as a high-dimensional vector in <span class="math inline">\(\mathbb{R}^n\)</span>, where <span class="math inline">\(n\)</span> is the same for all websites.</li>
<li>Calculate the similarities between these vectors.</li>
</ol>
<p>The first two points are concerned with data wrangling. In a machine learning context, the third step is the “learning step”, in which we learn numerical representations of our websites. This is actually the tricky part. Here any technique for document representation can be used. We will focus on a purely frequency-based approach, that is, in a nutshell, we will calculate which word appears how often in which document and weigh this frequency by the importance of the word. This will give us a long vector for each document and in the last step we then simply compare how these vectors are. This technique is part of the family of <em>term-frequency-inverse-document-frequency</em> (tf-idf) approaches and the details are given below. In particular, there is no actual “learning” involved in this approach as semantics and order of the text are not taken into account. In machine learning language, these representations are often called “document embeddings”, although it is to be noted, that they are <strong>not</strong> embeddings in the mathematical sense of injective maps, as reordering of words will result in the same numerical representation.</p>
<h1 id="getting-ourselves-a-batman">Getting ourselves a Batman</h1>
<p>The first item on our agenda is to obtain a copy of the Wikipedia pages of Batman, Robin and the Joker. In Python, the <code>requests</code> library allows us to do just that. Its use for obtaining the HTML from a website is fairly straightforward. For example, we can obtain the content of the page for Batman, which is located at <a href="https://en.wikipedia.org/wiki/Batman">https://en.wikipedia.org/wiki/Batman</a>, like so:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
import requests
url = &#39;https://en.wikipedia.org/wiki/Batman&#39;
batman = requests.get(url).text
print(batman[:2000])</code></pre>
<pre><code>
&lt;!DOCTYPE html&gt;
&lt;html class=&quot;client-nojs&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;
&lt;head&gt;
&lt;meta charset=&quot;UTF-8&quot;/&gt;
&lt;title&gt;Batman - Wikipedia&lt;/title&gt;
&lt;script&gt;document.documentElement.className=&quot;client-js&quot;;RLCONF={&quot;wgBreakFrames&quot;:!1,&quot;wgSeparatorTransformTable&quot;:[&quot;&quot;,&quot;&quot;],&quot;wgDigitTransformTable&quot;:[&quot;&quot;,&quot;&quot;],&quot;wgDefaultDateFormat&quot;:&quot;dmy&quot;,&quot;wgMonthNames&quot;:[&quot;&quot;,&quot;January&quot;,&quot;February&quot;,&quot;March&quot;,&quot;April&quot;,&quot;May&quot;,&quot;June&quot;,&quot;July&quot;,&quot;August&quot;,&quot;September&quot;,&quot;October&quot;,&quot;November&quot;,&quot;December&quot;],&quot;wgRequestId&quot;:&quot;9c470edb-1372-4be3-adcf-d91102c67c8f&quot;,&quot;wgCSPNonce&quot;:!1,&quot;wgCanonicalNamespace&quot;:&quot;&quot;,&quot;wgCanonicalSpecialPageName&quot;:!1,&quot;wgNamespaceNumber&quot;:0,&quot;wgPageName&quot;:&quot;Batman&quot;,&quot;wgTitle&quot;:&quot;Batman&quot;,&quot;wgCurRevisionId&quot;:965629643,&quot;wgRevisionId&quot;:965629643,&quot;wgArticleId&quot;:4335,&quot;wgIsArticle&quot;:!0,&quot;wgIsRedirect&quot;:!1,&quot;wgAction&quot;:&quot;view&quot;,&quot;wgUserName&quot;:null,&quot;wgUserGroups&quot;:[&quot;*&quot;],&quot;wgCategories&quot;:[&quot;Harv and Sfn multiple-target errors&quot;,&quot;Webarchive template wayback links&quot;,&quot;Harv and Sfn no-target errors&quot;,&quot;All articles with incomplete citations&quot;,&quot;Articles with incomplete citations from August 2016&quot;,&quot;Webarchive template warnings&quot;,&quot;Wikipedia articles needing page number citations from July 2016&quot;,
&quot;All articles with dead external links&quot;,&quot;Articles with dead external links from July 2016&quot;,&quot;CS1 maint: archived copy as title&quot;,&quot;CS1 Spanish-language sources (es)&quot;,&quot;Articles with dead external links from June 2017&quot;,&quot;Articles with permanently dead external links&quot;,&quot;Wikipedia indefinitely semi-protected pages&quot;,&quot;Wikipedia indefinitely move-protected pages&quot;,&quot;Articles with short description&quot;,&quot;Use mdy dates from October 2015&quot;,&quot;Character pop&quot;,&quot;Converted comics character infoboxes&quot;,&quot;Converted category character infoboxes&quot;,&quot;All articles lacking reliable references&quot;,&quot;Articles lacking reliable references from September 2019&quot;,&quot;Articles to be expanded from April 2019&quot;,&quot;All articles to be expanded&quot;,&quot;Articles using small message boxes&quot;,&quot;CS1 maint: ref=harv&quot;,&quot;Articles with Curlie links&quot;,&quot;Comics navigational boxes purge&quot;,&quot;Wikipedia articles with BNF identifiers&quot;,&quot;Wikipedia articles with GND identifie</code></pre>
</div>
<p>The <code>requests.get(url)</code> gets the content that is located at the specified URL for us. The returned object contains quite a bit more information than only the content located at the URL and to get to the actual HTML we use <code>requests.get(url).text</code>. If you are familiar with the structure of HTML or maybe even have read my previous blog posts on web-crawling (<a href="https://stefan-stein.github.io/posts/2020-01-09-webcrawling1/">here</a> and <a href="https://stefan-stein.github.io/posts/2020-02-09-webcrawling2/">here</a>), then it will come at no surprise that out <code>batman</code> object currently is still looking pretty messy: There are HTML tags all over the place and there is all sorts of information that is not part of the actual Wikipedia article. Luckily for us, there is the <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/"><code>BeautifulSoup</code></a> Python library specifically designed for obtaining information from HTML and XML files. Its documentation is pretty neat and the syntax needed is quite straightforward. Calling the <code>BeautifulSoup()</code> function on our <code>batman</code> returns a <code>BeautifulSoup</code> object, from which we can easily extract the parts of the HTML we are interested in. In the function call of <code>BeautifulSoup()</code> we need to specify how we want to parse the HTML by supplying the name of an HTML parser as a string. In this case we are using the <code>'lxml'</code> parser, which, according to the documentation is “very fast and lenient”, when it comes to parsing HTML. You might have to install <code>lxml</code> before using it by running</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="bash"><code>
$ pip install lxml</code></pre>
</div>
<p>from the terminal. See the <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser">BeautifulSoup documentation</a> for details. Once we have turned <code>batman</code> into <code>soup</code>, we can print the HTML using the <code>prettify()</code> function, which will take care of indentation and make the output more readable.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
from bs4 import BeautifulSoup
soup = BeautifulSoup(batman, &#39;lxml&#39;)
print(soup.prettify()[:2000])</code></pre>
<pre><code>
&lt;!DOCTYPE html&gt;
&lt;html class=&quot;client-nojs&quot; dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;
 &lt;head&gt;
  &lt;meta charset=&quot;utf-8&quot;/&gt;
  &lt;title&gt;
   Batman - Wikipedia
  &lt;/title&gt;
  &lt;script&gt;
   document.documentElement.className=&quot;client-js&quot;;RLCONF={&quot;wgBreakFrames&quot;:!1,&quot;wgSeparatorTransformTable&quot;:[&quot;&quot;,&quot;&quot;],&quot;wgDigitTransformTable&quot;:[&quot;&quot;,&quot;&quot;],&quot;wgDefaultDateFormat&quot;:&quot;dmy&quot;,&quot;wgMonthNames&quot;:[&quot;&quot;,&quot;January&quot;,&quot;February&quot;,&quot;March&quot;,&quot;April&quot;,&quot;May&quot;,&quot;June&quot;,&quot;July&quot;,&quot;August&quot;,&quot;September&quot;,&quot;October&quot;,&quot;November&quot;,&quot;December&quot;],&quot;wgRequestId&quot;:&quot;9c470edb-1372-4be3-adcf-d91102c67c8f&quot;,&quot;wgCSPNonce&quot;:!1,&quot;wgCanonicalNamespace&quot;:&quot;&quot;,&quot;wgCanonicalSpecialPageName&quot;:!1,&quot;wgNamespaceNumber&quot;:0,&quot;wgPageName&quot;:&quot;Batman&quot;,&quot;wgTitle&quot;:&quot;Batman&quot;,&quot;wgCurRevisionId&quot;:965629643,&quot;wgRevisionId&quot;:965629643,&quot;wgArticleId&quot;:4335,&quot;wgIsArticle&quot;:!0,&quot;wgIsRedirect&quot;:!1,&quot;wgAction&quot;:&quot;view&quot;,&quot;wgUserName&quot;:null,&quot;wgUserGroups&quot;:[&quot;*&quot;],&quot;wgCategories&quot;:[&quot;Harv and Sfn multiple-target errors&quot;,&quot;Webarchive template wayback links&quot;,&quot;Harv and Sfn no-target errors&quot;,&quot;All articles with incomplete citations&quot;,&quot;Articles with incomplete citations from August 2016&quot;,&quot;Webarchive template warnings&quot;,&quot;Wikipedia articles needing page number citations from July 2016&quot;,
&quot;All articles with dead external links&quot;,&quot;Articles with dead external links from July 2016&quot;,&quot;CS1 maint: archived copy as title&quot;,&quot;CS1 Spanish-language sources (es)&quot;,&quot;Articles with dead external links from June 2017&quot;,&quot;Articles with permanently dead external links&quot;,&quot;Wikipedia indefinitely semi-protected pages&quot;,&quot;Wikipedia indefinitely move-protected pages&quot;,&quot;Articles with short description&quot;,&quot;Use mdy dates from October 2015&quot;,&quot;Character pop&quot;,&quot;Converted comics character infoboxes&quot;,&quot;Converted category character infoboxes&quot;,&quot;All articles lacking reliable references&quot;,&quot;Articles lacking reliable references from September 2019&quot;,&quot;Articles to be expanded from April 2019&quot;,&quot;All articles to be expanded&quot;,&quot;Articles using small message boxes&quot;,&quot;CS1 maint: ref=harv&quot;,&quot;Articles with Curlie links&quot;,&quot;Comics navigational boxes purge&quot;,&quot;Wikipedia articles with BNF identifiers&quot;,&quot;Wikipedia articles w</code></pre>
</div>
<p>As we can see, the actual HTML content is still the same. The difference is, that Python now “understands” that this is HTML and not just one long messy string. In consequence, we can now quickly extract those parts of the HTML we are interested in. Before we can do that, though, we first need to find out where in the HTML the relevant information is located. The very easy to use <a href="https://selectorgadget.com/">SelectorGadget</a> Chrome extension is a great tool for this purpose. Simply by clicking on the sections of the website we want to extract, it will tell us for which CSS selector or HTML tag we need to filter our <code>soup</code> object. In our case, for example, we want all the content of the actual article, but not any of the sidebars/ header section of the page. It took me a bit of playing around to find that the solution in this case is quiet simple: All the relevant information to the task at hand is included in the paragraph (<code>&lt;p&gt;</code>) tags of the HTML.</p>
<p>Everything highlighted in yellow will be selected by filtering for the <code>&lt;p&gt;</code> tag. The parts in white will be discarded.</p>
<p><img src="batman_selector_1.png" /></p>
<p>If you are familiar with typical web page design, filtering for the <code>body</code> content of the page might seem like the more obvious initial choice. In that case, however, the sections References, Further Reading, External Links, in short: all the stuff at the bottom of the Wikipedia page, would also have been selected. When calculating the word frequencies later on, this would have disproportionally blown up the size of our wordlists as these sections contain hundreds of words and some words only appear here and nowhere else on the page. This would have distorted the word frequencies and associated word importances. When only filtering for <code>&lt;p&gt;</code> tags, these bottom sections of the page are excluded, as can be seen from the following screenshot.</p>
<p><img src="batman_selector_2.png" /></p>
<p>In BeautifulSoup, we can use the <code>find_all</code> method to extract the content of all the <code>&lt;p&gt;</code> tags. This will return a list with one entry for each paragraph, which we then can join back together into a single string using a list comprehension and <code>join()</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
content = soup.findAll(&#39;p&#39;)
content = &quot; &quot;.join([paragraph.get_text() for paragraph in content])
content[:70]</code></pre>
<pre><code>
&#39;\n \n Batman is a fictional character appearing in American comic books &#39;</code></pre>
</div>
<p>This is much better and the actual text is discernible. It is not yet exactly what we want, though, as there are still plenty of newline characters (<code>\n</code>) or unwanted numbers and parenthesis, which we will deal with in the next section. Using only paragraphs actually excludes all the headings. We might argue, though, that this is justified, as the headings should only reflect content of the subsequent paragraph anyways.</p>
<h1 id="tokenization-and-lemmatization">Tokenization and Lemmatization</h1>
<p>Aside from getting rid of the last stray unwanted characters, to be able to process this text and count word frequencies, we will have to break the text apart into single words. Machine Learners call this “tokenization”.</p>
<p>Also, words can appear in <a href="https://en.wikipedia.org/wiki/Inflection"><em>inflected forms</em></a>, that is, a word can be modified to reflect tense, case or gender. For example, “be”, “is”, “was”, are all forms of the same verb, “to be”, but each has been modified to reflect the tense in which it is being used. It can also happen that words are being used in a derivative form such as “democracy”, “democratic” and “democratization”. It would make sense, though, for calculating word frequencies to count all of them as the same word. Reducing these words to a single word is called <a href="https://en.wikipedia.org/wiki/Lemmatisation"><em>lemmatisation</em></a> and <a href="https://en.wikipedia.org/wiki/Stemming"><em>stemming</em></a> in linguistics. Also, there are plenty of words that do not carry any relevant information for us, such as “to”, “and”, “or”, “the”. These are called <em>stopwords</em> and we want to get rid of them before calculating our word frequencies.</p>
<p>While we could probably achieve tokenization with some regex magic, lemmatization and word stemming would be much harder to program from scratch, not to mention the tedious task of coming up with an exhaustive list of all the stopwords we want to remove. Again, luckily, there is a Python library out there dedicated to precisely such tasks: The famous <a href="https://www.nltk.org">Natural Language Toolkit, NLTK</a>. After installing NLTK, you will have to download the list of stopwords and word stemmer separately. You can do so directly from within Jupyter Notebook by running</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
import nltk
nltk.download(&#39;stopwords&#39;)
nltk.download(&#39;wordnet&#39;)</code></pre>
</div>
<p>In the following code snippet we import NLTK and initialize our word lemmatizer (<code>wnl</code>), word stemmer (<code>sno</code>), word tokenizer (<code>word_tokenizer</code>) and our list of stopwords (<code>stop_words</code>).</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
# tokenize and clean the text
import nltk
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from collections import Counter
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.tokenize import RegexpTokenizer
# tokenize anything that is not a number and not a symbol
word_tokenizer = RegexpTokenizer(r&#39;[^\d\W]+&#39;)

sno = SnowballStemmer(&#39;english&#39;)
wnl = WordNetLemmatizer()
# get our list of stop_words
stop_words = set(stopwords.words(&#39;english&#39;)) </code></pre>
</div>
<p>The list of stopwords contains common English words that do not carry any additional information:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
print(list(stop_words)[:10])</code></pre>
<pre><code>
[&#39;its&#39;, &#39;wasn&#39;, &#39;yourselves&#39;, &quot;that&#39;ll&quot;, &#39;were&#39;, &quot;don&#39;t&quot;, &#39;mightn&#39;, &#39;it&#39;, &#39;aren&#39;, &#39;that&#39;]</code></pre>
</div>
<p>Depending on the context, there might be some application specific words that we want to add to the list of stop words to exclude them from the analysis. In our case, for example, we can expect that the words “Gotham” or “comic” will appear in all three articles and they actually do not tell us much about who is more likely to whom. If we leave them in and, say, for some reason the word “Gotham” appears much more often in the articles about Batman and the Joker than in the article about Robin, this would indicate to our frequency based similarity measure that Batman and Joker are more similar to one another than to Robin. This might be a distortion of the true likeness between the three, though, as we would assume that “Gotham” appears in all three of the articles and does not actually tell us anything about them as a character. The different observed frequencies might simply be due to differing article lengths.</p>
<p>We can easily add custom stopwords like so:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
stop_words |= {&#39;gotham&#39;, &#39;comic&#39;}</code></pre>
</div>
<p>One could argue about whether these are all the words that should be excluded. Also, there are other ways of ignoring words that appear in all documents, see below. The point is, if you have specific words you want to add to your own stopwords list, this is how you do it.</p>
<p>Now, onto the actual cleaning of the text. Currently, we have the <code>content</code> object which is one long string with the content of the Wikipedia page. I am a big fan of Python list comprehensions, as they allow for clean, powerful and concise code. Much the same way that <code>lapply()</code> makes many for-loops redundant in <span class="math inline">\(\mathsf{R}\)</span>, list comprehensions let us collapse for-loops into a single line of code in Python. Therefore, we will now build up two list comprehensions that will take care of data cleaning for us. The first takes our messy <code>content</code> and returns a lemmatized version of it. The second one takes the lemmatized text and stems the words into their root form, using our word stemmer <code>sno</code>, returning a cleaned word list. Let’s go step by step.</p>
<p>First, we want to break this string up into single words, so the first building block is:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
[w for w in word_tokenizer.tokenize(content)]</code></pre>
</div>
<p>Then, we want to make sure that these words <code>w</code> are actual words and discard any special characters or numbers, i.e. want to check that <code>w.is.alpha() == True</code>. Secondly, we want to make sure, <code>w</code> is not one of our stopwords. Thus, we add to our list comprehension:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
[w for w in word_tokenizer.tokenize(content) if \
                w.isalpha() and w not in stop_words ]]</code></pre>
</div>
<p>Next, is lemmatization:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
lemm_txt = [ wnl.lemmatize(wnl.lemmatize(w.lower(),&#39;n&#39;),&#39;v&#39;) \
                for w in word_tokenizer.tokenize(content) if \
                w.isalpha() and w not in stop_words ]</code></pre>
</div>
<p>Finally, we add the word stemming. We also add another condition to only include such words that are at least three characters long, as we want to get rid of any accidentally created trailing words in our list (there are, of course, plenty of proper English words with only two characters, but these usually do not carry any relevant information, such as “to”, “or”, “me” etc.).</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
[ sno.stem(w) for w in lemm_txt if w not in stop_words and len(w) &gt; 2 ][:5]</code></pre>
<pre><code>
[&#39;batman&#39;, &#39;fiction&#39;, &#39;charact&#39;, &#39;appear&#39;, &#39;american&#39;]</code></pre>
</div>
<p>This looks about right! We can now wrap all of the text cleaning into a single function.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
def clean_text(txt):
    lemm_txt = [ wnl.lemmatize(wnl.lemmatize(w.lower(),&#39;n&#39;),&#39;v&#39;) \
                for w in word_tokenizer.tokenize(txt) if \
                w.isalpha() and w not in stop_words ]
    return [ sno.stem(w) for w in lemm_txt if w not in stop_words and len(w) &gt; 2 ]</code></pre>
</div>
<p>With this function and the code snippets from the previous section, we can also define a function <code>get_website()</code>, that, when given the name of our superhero/ villain, will retrieve and pre-process its Wikipedia page.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
def get_website(subject):
    url = &#39;https://en.wikipedia.org/wiki/&#39; + subject
    r_subject = requests.get(url).text
    soup = BeautifulSoup(r_subject, &#39;lxml&#39;)
    content = soup.findAll(&#39;p&#39;)
    content = &quot; &quot;.join([paragraph.get_text() for paragraph in content])
    return clean_text(content)</code></pre>
</div>
<p>Getting the website content for our three characters, now is just - you guessed it - a single list comprehension away.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
heroes = [&#39;Batman&#39;, &#39;Robin_(character)&#39;, &#39;Joker_(character)&#39;]
hero_websites = [get_website(hero) for hero in heroes]</code></pre>
</div>
<p>Let’s take a look at Robin, say,</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
hero_websites[1][:20]</code></pre>
<pre><code>
[&#39;robin&#39;, &#39;name&#39;, &#39;sever&#39;, &#39;fiction&#39;, &#39;superhero&#39;, &#39;appear&#39;, &#39;american&#39;, &#39;book&#39;, &#39;publish&#39;, &#39;charact&#39;, &#39;origin&#39;, &#39;creat&#39;, &#39;bob&#39;, &#39;kane&#39;, &#39;bill&#39;, &#39;finger&#39;, &#39;jerri&#39;, &#39;robinson&#39;, &#39;serv&#39;, &#39;junior&#39;]</code></pre>
</div>
<p>Alright! These lists still contain duplicates. So what we want to do next is count which word appears how many times in each website. That is, for each character we want to create a dictionary of with entries of the form “<code>word : frequency</code>”. This is called a <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words model</a>. The <code>Counter</code> function from the <code>collections</code> Python library does exactly that. We will have to print quite a few dictionaries further down, so I introduce a helper function here, that allows for prettier printing of dictionaries. <code>print_sorted()</code> will sort a dictionary based on its values, i.e. the word frequencies in our case, and print the top <code>number</code> entries (10 by default). Let’s try this for the Joker.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
from collections import Counter

# useful function to print a dictionary sorted by value (largest first by default)
def print_sorted(d, ascending=False, number = 10):
    factor = 1 if ascending else -1
    sorted_list = sorted(d.items(), key=lambda v: factor*v[1])
    for i, v in sorted_list[:number]:
        print(&quot;{}: {:.3f}&quot;.format(i, v))

print_sorted(Counter(hero_websites[2]))</code></pre>
<pre><code>
joker: 279.000
batman: 185.000
charact: 98.000
stori: 48.000
villain: 40.000
kill: 38.000
seri: 37.000
appear: 33.000
death: 27.000
book: 26.000</code></pre>
</div>
<p>Looks about right. To end this section, let’s get some summary statistics for each hero: How many and how many unique words does each of their websites contain?</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
for i in range(len(heroes)):
    print(&#39;Size of {} website: {} words and {} unique words&#39;.format(heroes[i], 
                                                                    len(hero_websites[i]), 
                                                                    len(Counter(hero_websites[i]))))</code></pre>
<pre><code>
Size of Batman website: 7769 words and 2125 unique words
Size of Robin_(character) website: 2441 words and 917 unique words
Size of Joker_(character) website: 5009 words and 1651 unique words</code></pre>
</div>
<h1 id="term-frequencies-and-inverse-document-frequencies">Term frequencies and inverse document frequencies</h1>
<p>We see that both in terms of overall size as well as number of unique words, the websites differ quite substantially. Therefore, we need to assign each word its character-specific weight. Otherwise, if we assigned the word “Batman”, say, the same weight for all characters, then the Joker might seem much more like Batman, simply because their websites are longer and thus have more occurrences of the word “Batman”. The “term frequency, inverse document frequency”-approaches, or “tf-idf”-approaches for short, do just that. The exact formulas differ and are up to personal preference, but the basic idea is always the following: We have a bag-of-words dictionary for each website, as well as a dictionary of how often words appear in all of the websites together. For each word <span class="math inline">\(w\)</span> in each document <span class="math inline">\(d\)</span> we now calculate the <em>term frequency</em>, <span class="math inline">\({\rm tf}(w,d)\)</span>, which measures how often <span class="math inline">\(w\)</span> appears in <span class="math inline">\(d\)</span>. Again, there are different formulas for doing this, but the general idea is, that <span class="math inline">\({\rm tf}(w,d)\)</span> measures how important <span class="math inline">\(w\)</span> is for <span class="math inline">\(d\)</span>. Next, we calculate the <em>inverse document frequency</em>, <span class="math inline">\({\rm idf}(w)\)</span>, for each word <span class="math inline">\(w\)</span>. The general idea of <span class="math inline">\(\rm idf\)</span> is that we count how many documents contain the word <span class="math inline">\(w\)</span>, call this <span class="math inline">\(\text{df}_w\)</span>, and then take <span class="math inline">\(\text{idf}(w)\)</span> to be some transformation of <span class="math inline">\(N/\text{df}_w\)</span>, where <span class="math inline">\(N\)</span> is the total number of documents we have. That is, if <span class="math inline">\(w\)</span> appears in many of the documents, <span class="math inline">\(\text{idf}(w)\)</span> will be small, signaling that <span class="math inline">\(w\)</span> is not not a very distinctive word. If, on the other hand, <span class="math inline">\(w\)</span> appears only in very few documents or is unique to a specific document, <span class="math inline">\(\text{idf}(w)\)</span> will be large, signaling that containing <span class="math inline">\(w\)</span> is a distinctive feature for a document.</p>
<p>Once we have computed <span class="math inline">\(\text{tf}(w,d)\)</span> for each word <span class="math inline">\(w\)</span> and document <span class="math inline">\(d\)</span> and <span class="math inline">\(\text{idf}(w)\)</span> for each word <span class="math inline">\(w\)</span>, we calculate the vector representation of <span class="math inline">\(d\)</span> as the vector with entries <span class="math display">\[
  v_d = (\text{tf}(w,d) \cdot \text{idf}(w))_{w \in \mathcal{C}},
\]</span> where <span class="math inline">\(\mathcal{C}\)</span> is our <em>corpus</em>, i.e. all the unique words of all documents. This means, the dimension of our representation space for the documents is <span class="math inline">\(c = \vert \mathcal{C} \vert\)</span>, i.e. we represent each document <span class="math inline">\(d\)</span> as a vector in <span class="math inline">\(\mathbb{R}^c\)</span>. Notice a few things</p>
<ol type="1">
<li>The dimension <span class="math inline">\(c\)</span> is the same for all documents. Otherwise we would not be able to calculate similarities between document representations.</li>
<li>For most documents, the dimension <span class="math inline">\(c\)</span> will be strictly larger than the number of unique words in each document, i.e. there will be words in <span class="math inline">\(\mathcal{C}\)</span> that don’t appear in <span class="math inline">\(d\)</span> and by default we set their entries in <span class="math inline">\(v_d\)</span> to zero.</li>
<li>In our notation above we implicitly assume that there is a fixed ordering to our words in <span class="math inline">\(\mathcal{C}\)</span>, such that the <span class="math inline">\(k\)</span>-th entry of <span class="math inline">\(v_d\)</span> will always correspond to the same word <span class="math inline">\(w\)</span> for all documents <span class="math inline">\(d\)</span>.</li>
</ol>
<p>We will need to make sure that the last two points are reflected in our code. Let’s get into the actual maths and code!</p>
<h2 id="computing-idf">Computing idf</h2>
<p>Given our list of bag-of-words for each DC character, we want to compute for each word <span class="math inline">\(w\)</span>, the inverse-document-frequency, or <span class="math inline">\({\rm idf}(w)\)</span>. This can be done in a few steps:</p>
<ol type="1">
<li>Gather a set of all the words in all the bag-of-words (recall the union operator <code>|</code> from before which will come in handy here).</li>
<li>Loop over each word <span class="math inline">\(w\)</span>, and compute <span class="math inline">\({\rm df}_w\)</span>, the number of documents in which this word appears at least once: <span class="math display">\[
  \text{df}_w = \sum_{i = 1}^N I(w \in d_i),
\]</span> where <span class="math inline">\(N\)</span> is the number of documents we have and <span class="math inline">\(I\)</span> is the indicator function.</li>
<li>After computing <span class="math inline">\({\rm df}_w\)</span>, we can compute <span class="math inline">\({\rm idf}(w)\)</span>. There are usually two possibilities, the simplest one is <span class="math display">\[{\rm idf}(w)=\frac{N}{{\rm df}_w}.\]</span> Frequently, a logarithm term is added for stabilization <span class="math display">\[{\rm idf}(w)=\log\frac{N}{{\rm df}_w}.\]</span> One possible issue with using the logarithm is that when <span class="math inline">\({\rm df}_w = N\)</span>, <span class="math inline">\({\rm idf}(w)=0\)</span>, indicating that words common to all documents would be ignored, which can be undesirable depending on the context. If we don’t want this behavior, we can define <span class="math inline">\({\rm idf}(w)=\log\left(1+N/{\rm df}_w\right)\)</span> or <span class="math inline">\({\rm idf}(w)=1+\log\left(N/{\rm df}_w\right)\)</span> instead. In our case we only have three documents in total, so censoring out all the words that appear in all three of them might be throwing away a lot of information, which is why we add the <span class="math inline">\(+1\)</span> shift in our calculations.</li>
</ol>
<p>In the following, we define a function called <code>get_idf(corpus, include_log=True)</code> that computes <span class="math inline">\({\rm idf}(w)\)</span> for all the words in a corpus, where <code>corpus</code> for us is a processed list of bag-of-words (stemmed and lemmatized). The optional parameter <code>include_log</code> includes the logarithm in the computation.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
# compute idf
from math import log
from collections import defaultdict

def get_idf(corpus, include_log=True):
    N = len(corpus)
    freq = defaultdict(int)
    words = set()
    for c in corpus:
        words |= set(c)
        
    for w in words:
        freq[w] = sum([ w in c for c in corpus])

    if include_log:
        return { w:1 + log(N/freq[w]) for w in freq }
    else:
        return { w:N/freq[w] for w in freq }</code></pre>
</div>
<p>Let’s go through this code line by line: The argument <code>corpus</code> will be the list of bag-of-words for each document. We set <code>N</code> to be the length of <code>corpus</code>, i.e. the number of documents we have. Then, we initialize a dictionary <code>freq</code>, which for each word <code>w</code> will store in how many documents <code>w</code> appears. Notice the use of <code>defaultdict</code> here, which will become important later. The first for-loop creates a set with all the unique words in all documents. The second for-loop iterates over each word <code>w</code> in our set of unique words, <code>words</code>: The expression “<code>w in c</code>” evaluates to <code>True</code> if <code>w</code> appears in the document <code>c</code> and <code>False</code> otherwise, i.e. we are counting in how many documents <code>w</code> appears. Finally, a dictionary of inverse document frequencies is returned.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
idf = get_idf(hero_websites)</code></pre>
</div>
<h2 id="computing-tf">Computing tf</h2>
<p>Below we will compute <span class="math inline">\({\rm tf}(w,d)\)</span>, or the term frequency for each word <span class="math inline">\(w\)</span>, and each document <span class="math inline">\(d\)</span>. There are multiple definitions for <span class="math inline">\({\rm tf}(w,d)\)</span>, the simplest one is</p>
<p><span class="math display">\[
{\rm tf}(w,d)=\frac{f_{w,d}}{a_d}
\]</span></p>
<p>where <span class="math inline">\(f_{w,d}\)</span> is the number of occurrence of the word <span class="math inline">\(w\)</span> in the document <span class="math inline">\(d\)</span>, and <span class="math inline">\(a_d\)</span> the average occurrence of all the words in that document for normalization. Just like <span class="math inline">\({\rm idf}(w)\)</span>, a logarithm can be added</p>
<p><span class="math display">\[
{\rm tf}(w,d)=\begin{cases}
\frac{1+\log f_{w,d}}{1+\log a_d} &amp; f_{w,d} &gt; 0, \\
0 &amp; f_{w,d} = 0. \\
\end{cases}
\]</span></p>
<p>We implement the function <code>get_tf(txt, include_log=True)</code> that computes <span class="math inline">\({\rm tf}(w,d)\)</span> for each word in the document. The function also includes the optional parameter <code>include_log</code> that enables the additional logarithm term in the computation. The code is rather straightforward. For better readability, we introduce the helper function <code>_tf()</code> that does the actual computation.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
import numpy as np
from math import *

def _tf(freq, avg, include_log=True):
    if include_log:
        return 0 if freq == 0 else (1+log(freq))/(1+log(avg))
    else:
        return freq/avg

def get_tf(txt, include_log=True):
    freq = Counter(txt)
    avg = np.mean(list(freq.values()))
    tf = {w:_tf(f,avg, include_log) for w,f in freq.items()}
    return defaultdict(int, tf)</code></pre>
</div>
<p>Given a word frequency and an average word frequency, the function <code>_tf</code> returns the value for <span class="math inline">\(\text{tf}(w,d)\)</span>. The function <code>get_tf()</code> takes our document (<code>txt</code>), calculates the frequencies of each word (<code>Counter(txt)</code>) and the average word frequency (<code>avg</code>). It then creates a dictionary with the unique words as values and their <span class="math inline">\(\text{tf}(w,d)\)</span> values as keys. Again, note the use of <code>defaultdict</code> here, which will come into play in the next step.</p>
<p>For now, we can get the term frequencies for each document using another simple list comprehension.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
tfs = [ get_tf(c) for c in hero_websites ]</code></pre>
</div>
<p>Let’s take a look what this returns, for example for our Batman page.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
print_sorted(tfs[0])</code></pre>
<pre><code>
batman: 3.131
wayn: 2.506
bruce: 2.462
charact: 2.441
seri: 2.260
stori: 2.240
bat: 2.173
robin: 2.164
also: 2.093
film: 2.042</code></pre>
</div>
<p>Nice! This is what we would expect. The words “batman”, “wayn” (stemmed form of “Wayne”) and “bruce” appear most frequently.</p>
<p>Now we have all the components to calculate the tf-idf vector for each document: Given the dictionary of term frequencies for a document, <code>tf</code>, and the dictionary of inverse document frequencies, <code>idf</code>, we just need to calculate <code>tf[w]*idf[w]</code> for each word <code>w</code> in <code>idf</code>. It is important to keep in mind that we have to iterate over all words in <code>idf</code>: <code>idf</code> contains all the words in all documents, while <code>tf</code> only contains those words specific to the document at hand. Recall that we want to represent each document as a numeric vector <em>of the same dimension</em> and that dimension will be the size of our corpus <span class="math inline">\(\mathcal{C}\)</span>, which is precisely the length of the dictionary <code>idf</code>. So naturally there will be words contained in <code>idf</code> that do not appear in <code>tf</code>. Thus, when calculating the tf-idf vector we need to add zero-padding for words that do not appear in our document. Say, <code>w</code> is one such word. Had we initialized our <code>tf</code> as a regular dictionary, trying to call <code>tf[w]</code> would throw an error. This is precisely why we initialized our <code>tf</code> as an instance of <code>defaultdict(int)</code>. When a <code>defaultdict</code> is called with a key that is currently not stored in it, it will automatically initialize this key with a default value. Since we specified <code>defaultdict(int)</code>, this default value will be zero.</p>
<p>The final code is now a simple one liner.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
def get_vector(tf, idf):
    # idf is longer (dict for all words in all documents), hence iterate over idf
    return np.array([ tf[w]*idf[w] for w in idf ])</code></pre>
</div>
<p>Now, we can easily calculate the tf-idf vector for each document. Let’s also make sure that indeed, all vectors have the same dimension.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
doc_vectors = [ get_vector(tf, idf) for tf in tfs ]

for v in doc_vectors:
    print(len(v))</code></pre>
<pre><code>
3099
3099
3099</code></pre>
</div>
<p>Nice! Exactly as desired.</p>
<h1 id="similarity-measures">Similarity measures</h1>
<p>Given two numeric representation vectors <span class="math inline">\(u, v \in \mathbb{R}^c\)</span> with components <span class="math inline">\(u_i\)</span> and <span class="math inline">\(v_i, i = 1, \dots, c\)</span> respectively, corresponding to two documents, we want to compute a similarity metric. A commonly used similarity metric is the so-called <a href="https://en.wikipedia.org/wiki/Jaccard_index"><em>Jaccard similarity</em></a>. It is defined as follows.</p>
<p><span class="math display">\[
{\rm Sim}_{\rm Jaccard} = \frac{\sum_i \min\{u_i, v_i\}}{\sum_i \max\{u_i, v_i\}}.
\]</span></p>
<p>Implementing this function is a short one-liner.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
# Jaccard similarity

def sim_jac(u,v):
    return sum(np.minimum(u,v))/sum(np.maximum(u,v))</code></pre>
</div>
<p>Now we can calculate the similarity matrices between our three superheroes/ villains.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="python"><code>
# compute all the pairwise similarity metrics
size = len(doc_vectors)
matrix_jac = np.zeros((size,size))

for i in range(size):
    for j in range(size):
        u = doc_vectors[i]
        v = doc_vectors[j]
        matrix_jac[i][j] = sim_jac(u,v)
        </code></pre>
</div>
<p>So, what did we get? To get a pretty representation of the similarity matrix, we can plot a heatmap of the obtained similarity scores. The code is adapted directly from the official <a href="https://matplotlib.org/3.1.1/gallery/images_contours_and_fields/image_annotated_heatmap.html">matplotlib website</a>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre><code>
[&lt;matplotlib.axis.XTick object at 0x1a2a69c190&gt;, &lt;matplotlib.axis.XTick object at 0x1a2a8ea890&gt;, &lt;matplotlib.axis.XTick object at 0x1a2a8eac10&gt;]</code></pre>
<pre><code>
[&lt;matplotlib.axis.YTick object at 0x1a2a69cb10&gt;, &lt;matplotlib.axis.YTick object at 0x1a2a69c990&gt;, &lt;matplotlib.axis.YTick object at 0x1a2a8eab50&gt;]</code></pre>
<pre><code>
[Text(0, 0, &#39;Batman&#39;), Text(0, 0, &#39;Robin&#39;), Text(0, 0, &#39;Joker&#39;)]</code></pre>
<pre><code>
[Text(0, 0, &#39;Batman&#39;), Text(0, 0, &#39;Robin&#39;), Text(0, 0, &#39;Joker&#39;)]</code></pre>
<pre><code>
[None, None, None, None, None, None]</code></pre>
<p><img src="websitesimilarities_files/figure-html5/heatmap-1.png" width="960" /></p>
</div>
<p>So, finally, what is the answer to our research question “Is Batman more like the Joker or more like Robin?”? The (shocking!) truth is: Batman is more like the Joker! Oh dear… can you imagine the headlines when this gets out into the world? Our favorite and iconic superhero being lumped in together with one of the most notorious villains of all times?</p>
<p>On a more serious note, we see that while the similarity is greatest between Batman and Joker, with a value of <span class="math inline">\(0.24\)</span> it is still not all that great. Also, it is debatable whether the vector representations of the websites are the best possible. There might be a greater likeness between the Batman page and the Joker page, simply because they are longer than the Robin page and important words thus appear more often on them. Also, the tf-idf approach only takes word frequencies into account, not word ordering or semantics. Since the stories of the Joker and Batman are very entwined, it makes sense that a lot of the same words or accounts of important events would appear in both pages. But just using the same words, does not yet mean that the meaning of the resulting sentences are actually the same. Although there are these obvious shortcomings for our particular use-case, the techniques discussed are widely used in the ML and NLP community and have quite a few merits. Should you ever want to create your own project of this type, I would be happy if the code provided could serve you as an inspiration. All in all, I hope you enjoyed this post and thanks for reading until here!</p>
<p>Have comments or suggestions? Let me know!</p>
<h4 id="credits-for-the-preview-image">Credits for the preview image</h4>
<p>Lego is owned by The Lego Group. I am not associated with The Lego Group in any way and do not own the image used. I do not have any financial gains from using this image.</p>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="updates-and-corrections">Corrections</h3>
<p>If you see mistakes or want to suggest changes, please <a href="https://github.com/stefan-stein/website_similarites/issues/new">create an issue</a> on the source repository.</p>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
