---
title: "Webcrawling with R part 1: Teaching rvest how to crawl"
description: |
  rvest is a great package for downloading information from the internet using R. However, it does not natively
  "crawl" the web, i.e. traverse from one website to another. So let's teach it!
author:
  - name: Stefan Stein
    url: {}
date: 01-09-2020
categories:
  - webcrawling
  - R
output:
  distill::distill_article:
    self_contained: false
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


When I was a kid, the only way to access the internet I knew was through a web browser. You opened it up and sort of like stepping through a portal you went into the world of the internet, that was somewhere "out there".
When I heard about web-crawlers, programs that systematically traverse and analyse the web, for the first time, I imagined a program that would open up your browser and load one website after another, dozens of tabs at a time, at a speed too fast for the human eye to follow. I was awestruck by the mere thought of it! As you might imagine, that’s not quite how it works. As you might also imagine, I didn’t have the slightest idea about how the internet worked. Well, a decade or so later, I still don’t really know how the internet works deep down, but I recently found out that I do know enough R to build my own web-crawler. In this blogpost I will share my experiences with you.

Our main tool will be the package `rvest`, [@rvest] by Hadley Wickham. This is an excellent package for obtaining content from a specific webpage given its URL. However, `rvest` does not natively crawl, that is, it does not traverse from one webpage to the next. So our goal will be to implement this crawling mechanism. As a case study we will start with the Wikipedia page for "squirrel" and systematically visit all the other Wikipedia pages that can be reached with no more than two clicks. Mathematically speaking, we are extracting the 2-neighbourhood of the hyperlink network of Wikipedia's "squirrel" page.

**A little brain teaser before reading on:** What do you think how many (English) Wikipedia pages you can reach with just two clicks starting from the entry for "squirrel"?

### I will talk about

- Obtaining content of a website (using `rvest`, [@rvest])
- Filtering that content for specific data (using `rvest`)
- Teaching `rvest` how to crawl (using a `while loop` and `regular expressions`)
- Making pretty graphs (using `igraph`, [@igraph])

### General strategy

Our general strategy will be

- Start with an initial list of urls (we will talk about how to get them later),
- Visit each url, extract name of website owner and all the hyperlinks to other people in the department, add new hyperlinks to our list of urls we want to visit,
- Keep going until we have exhausted our list.

This works, because all personal websites have the same layout (i.e. the name will always be located in the same place) and the hyperlinks to people in the department all have the same structure (i.e. include "staff" or "research-student" etc.).

