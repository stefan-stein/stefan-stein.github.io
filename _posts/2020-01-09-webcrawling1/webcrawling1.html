<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

  <!--radix_placeholder_meta_tags-->
  <title>Web crawling with R part 1: Teaching rvest how to crawl</title>
  
  <meta property="description" itemprop="description" content="rvest is a great package for downloading information from the internet straight into your R session. &#10;However, it does not natively &quot;crawl&quot; the web, i.e. traverse from one website to another. So let&#39;s teach it!&#10;The code and the extracted hyperlink network data can be found in the corresponding [GitHub repository](https://github.com/stefan-stein/Wikipedia_webcrawling)."/>
  
  
  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2020-02-01"/>
  <meta property="article:created" itemprop="dateCreated" content="2020-02-01"/>
  <meta name="article:author" content="Stefan Stein"/>
  
  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Web crawling with R part 1: Teaching rvest how to crawl"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="rvest is a great package for downloading information from the internet straight into your R session. &#10;However, it does not natively &quot;crawl&quot; the web, i.e. traverse from one website to another. So let&#39;s teach it!&#10;The code and the extracted hyperlink network data can be found in the corresponding [GitHub repository](https://github.com/stefan-stein/Wikipedia_webcrawling)."/>
  <meta property="og:locale" content="en_US"/>
  
  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Web crawling with R part 1: Teaching rvest how to crawl"/>
  <meta property="twitter:description" content="rvest is a great package for downloading information from the internet straight into your R session. &#10;However, it does not natively &quot;crawl&quot; the web, i.e. traverse from one website to another. So let&#39;s teach it!&#10;The code and the extracted hyperlink network data can be found in the corresponding [GitHub repository](https://github.com/stefan-stein/Wikipedia_webcrawling)."/>
  
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=rvest: Easily Harvest (Scrape) Web Pages;citation_publication_date=2016;citation_author=Hadley Wickham"/>
  <meta name="citation_reference" content="citation_title=The igraph software package for complex network research;citation_publication_date=2006;citation_volume=Complex Systems;citation_author=Gabor Csardi;citation_author=Tamas Nepusz"/>
  <!--radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","author","date","categories","output","bibliography","repository_url","preview"]}},"value":[{"type":"character","attributes":{},"value":["Web crawling with R part 1: Teaching rvest how to crawl"]},{"type":"character","attributes":{},"value":["rvest is a great package for downloading information from the internet straight into your R session. \nHowever, it does not natively \"crawl\" the web, i.e. traverse from one website to another. So let's teach it!\nThe code and the extracted hyperlink network data can be found in the corresponding [GitHub repository](https://github.com/stefan-stein/Wikipedia_webcrawling).\n"]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","url"]}},"value":[{"type":"character","attributes":{},"value":["Stefan Stein"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":[]}},"value":[]}]}]},{"type":"character","attributes":{},"value":["02-01-2020"]},{"type":"character","attributes":{},"value":["webcrawling","R"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["distill::distill_article"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["self_contained"]}},"value":[{"type":"logical","attributes":{},"value":[false]}]}]},{"type":"character","attributes":{},"value":["bibliography.bib"]},{"type":"character","attributes":{},"value":["https://github.com/stefan-stein/Wikipedia_webcrawling"]},{"type":"character","attributes":{},"value":["https://en.wikipedia.org/wiki/Squirrel#/media/File:Sciuridae.jpg"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["bibliography.bib","squirrel_inspectelemet.png","webcrawling1_files/bowser-1.9.3/bowser.min.js","webcrawling1_files/distill-2.2.21/template.v2.js","webcrawling1_files/jquery-1.11.3/jquery.min.js","webcrawling1_files/webcomponents-2.0.0/webcomponents.js"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->
  
  <style type="text/css">
  
  body {
    background-color: white;
  }
  
  .pandoc-table {
    width: 100%;
  }
  
  .pandoc-table>caption {
    margin-bottom: 10px;
  }
  
  .pandoc-table th:not([align]) {
    text-align: left;
  }
  
  .pagedtable-footer {
    font-size: 15px;
  }
  
  .html-widget {
    margin-bottom: 2.0em;
  }
  
  .l-screen-inset {
    padding-right: 16px;
  }
  
  .l-screen .caption {
    margin-left: 10px;
  }
  
  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .shaded .shaded-content {
    background: white;
  }
  
  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }
  
  .hidden {
    display: none !important;
  }
  
  d-article {
    padding-bottom: 30px;
  }
  
  d-appendix {
    padding-top: 30px;
  }
  
  d-article>p>img {
    width: 100%;
  }
  
  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }
  
  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  /* CSS for table of contents */
  
  .d-toc {
    color: rgba(0,0,0,0.8);
    font-size: 0.8em;
    line-height: 1em;
  }
  
  .d-toc-header {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    text-transform: uppercase;
    margin-top: 0;
    margin-bottom: 1.3em;
  }
  
  .d-toc a {
    border-bottom: none;
  }
  
  .d-toc ul {
    padding-left: 0;
  }
  
  .d-toc li>ul {
    padding-top: 0.8em;
    padding-left: 16px;
    margin-bottom: 0.6em;
  }
  
  .d-toc ul,
  .d-toc li {
    list-style-type: none;
  }
  
  .d-toc li {
    margin-bottom: 0.9em;
  }
  
  .d-toc-separator {
    margin-top: 20px;
    margin-bottom: 2em;
  }
  
  .d-article-with-toc {
    border-top: none;
    padding-top: 0;
  }
  
  
  
  /* Tweak code blocks (note that this CSS is repeated above in an injection
     into the d-code shadow dom) */
  
  d-code {
    overflow-x: auto !important;
  }
  
  pre.d-code code.d-code {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }
  
  pre.text-output {
  
    font-size: 12px;
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  @media(min-width: 768px) {
  
  d-code {
    overflow-x: visible !important;
  }
  
  pre.d-code code.d-code  {
      padding-left: 18px;
      font-size: 14px;
  }
  pre.text-output {
    font-size: 14px;
  }
  }
  
  /* Figure */
  
  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }
  
  .figure img {
    width: 100%;
  }
  
  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }
  
  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }
  
  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }
  
  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }
  
  
  
  /* Tweak 1000px media break to show more text */
  
  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }
  
    .grid {
      grid-column-gap: 16px;
    }
  
    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }
  
  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }
  
    .grid {
      grid-column-gap: 32px;
    }
  }
  
  
  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */
  
  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }
  
  
  /* Social footer */
  
  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }
  
  .disqus-comments {
    margin-right: 30px;
  }
  
  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }
  
  #disqus_thread {
    margin-top: 30px;
  }
  
  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }
  
  .article-sharing a:hover {
    border-bottom: none;
  }
  
  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }
  
  .subscribe p {
    margin-bottom: 0.5em;
  }
  
  
  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }
  
  
  /* Improve display for browsers without grid (IE/Edge <= 15) */
  
  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }
  
  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }
  
  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }
  
  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }
  
  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }
  
  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }
  
  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }
  
  
  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }
  
  .downlevel .footnotes ol {
    padding-left: 13px;
  }
  
  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }
  
  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }
  
  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }
  
  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;
  
    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;
  
    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }
  
  </style>
  
  <script type="application/javascript">
  
  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }
  
  // show body when load is complete
  function on_load_complete() {
  
    // set body to visible
    document.body.style.visibility = 'visible';
  
    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }
  
    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }
  
  function init_distill() {
  
    init_common();
  
    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);
  
    // create d-title
    $('.d-title').changeElementType('d-title');
  
    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);
  
    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();
  
    // move posts container into article
    $('.posts-container').appendTo($('d-article'));
  
    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');
  
    // create d-bibliography
    var bibliography = $('<d-bibliography></d-bibliography>');
    $('#distill-bibliography').wrap(bibliography);
  
    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;
  
    // replace citations with <d-cite>
    $('.citation').each(function(i, val) {
      appendix = true;
      var cites = $(this).attr('data-cites').split(" ");
      var dt_cite = $('<d-cite></d-cite>');
      dt_cite.attr('key', cites.join());
      $(this).replaceWith(dt_cite);
    });
    // remove refs
    $('#refs').remove();
  
    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();
  
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-toc a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });
  
    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');
  
    // replace code blocks with d-code
    $('pre>code').each(function(i, val) {
      var code = $(this);
      var pre = code.parent();
      var clz = "";
      var language = pre.attr('class');
      if (language) {
        // map unknown languages to "clike" (without this they just dissapear)
        if ($.inArray(language, ["bash", "clike", "css", "go", "html",
                                 "javascript", "js", "julia", "lua", "markdown",
                                 "markup", "mathml", "python", "svg", "xml"]) == -1)
          language = "clike";
        language = ' language="' + language + '"';
        var dt_code = $('<d-code block' + language + clz + '></d-code>');
        dt_code.text(code.text());
        pre.replaceWith(dt_code);
      } else {
        code.addClass('text-output').unwrap().changeElementType('pre');
      }
    });
  
    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {
  
      // capture layout
      var layout = $(this).attr('data-layout');
  
      // apply layout to markdown level block elements
      var elements = $(this).children().not('d-code, pre.text-output, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });
  
  
      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });
  
    // load distill framework
    load_distill_framework();
  
    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {
  
      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;
  
      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');
  
      // table of contents
      if (have_authors) // adjust border if we are in authors
        $('.d-toc').parent().addClass('d-article-with-toc');
  
      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');
  
      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }
  
      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');
  
      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");
  
      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }
  
       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }
  
      // inject pre code styles (can't do this with a global stylesheet b/c a shadow root is used)
      $('d-code').each(function(i, val) {
        var style = document.createElement('style');
        style.innerHTML = 'pre code { padding-left: 10px; font-size: 12px; border-left: 2px solid rgba(0,0,0,0.1); } ' +
                          '@media(min-width: 768px) { pre code { padding-left: 18px; font-size: 14px; } }';
        if (this.shadowRoot)
          this.shadowRoot.appendChild(style);
      });
  
      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();
  
      // clear polling timer
      clearInterval(tid);
  
      // show body now that everything is ready
      on_load_complete();
    }
  
    var tid = setInterval(distill_post_process, 50);
    distill_post_process();
  
  }
  
  function init_downlevel() {
  
    init_common();
  
     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));
  
    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;
  
    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();
  
    // remove toc
    $('.d-toc-header').remove();
    $('.d-toc').remove();
    $('.d-toc-separator').remove();
  
    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });
  
  
    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);
  
    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);
  
    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();
  
    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();
  
    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));
  
    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });
  
    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));
  
    $('body').addClass('downlevel');
  
    on_load_complete();
  }
  
  
  function init_common() {
  
    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};
  
        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });
  
        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);
  
    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});
  
    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });
  
      }
    });
  
    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });
  
    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace("index.html", "./"));
      });
    }
  
    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');
  
    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");
  
    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();
  
    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }
  
  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });
  
  </script>
  
  <!--/radix_placeholder_distill-->
  <script src="webcrawling1_files/jquery-1.11.3/jquery.min.js"></script>
  <script src="webcrawling1_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="webcrawling1_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="webcrawling1_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Web crawling with R part 1: Teaching rvest how to crawl","description":"rvest is a great package for downloading information from the internet straight into your R session. \nHowever, it does not natively \"crawl\" the web, i.e. traverse from one website to another. So let's teach it!\nThe code and the extracted hyperlink network data can be found in the corresponding [GitHub repository](https://github.com/stefan-stein/Wikipedia_webcrawling).","authors":[{"author":"Stefan Stein","authorURL":{},"affiliation":"&nbsp;","affiliationURL":"#"}],"publishedDate":"2020-02-01T00:00:00.000+00:00","citationText":"Stein, 2020"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Web crawling with R part 1: Teaching rvest how to crawl</h1>
<p>rvest is a great package for downloading information from the internet straight into your R session. However, it does not natively “crawl” the web, i.e. traverse from one website to another. So let’s teach it! The code and the extracted hyperlink network data can be found in the corresponding <a href="https://github.com/stefan-stein/Wikipedia_webcrawling">GitHub repository</a>.</p>
</div>

<div class="d-byline">
  Stefan Stein true 
  
<br/>02-01-2020
</div>

<div class="d-article">
<p>When I was a kid, the only way to access the internet I knew was through a web browser. You opened it up and sort of like stepping through a portal you went into the world of the internet, that was somewhere “out there”. When I heard about web-crawlers, programs that systematically traverse and analyse the web, for the first time, I imagined a program that would open up your browser and load one website after another, dozens of tabs at a time, at a speed too fast for the human eye to follow. I was awestruck by the mere thought of it! As you might imagine, that’s not quite how it works. As you might also imagine, I didn’t have the slightest idea about how the internet worked. Well, a decade or so later, I still don’t really know how the internet works deep down, but I recently found out that I do know enough R to build my own web-crawler. In this blog post I will share my experiences with you.</p>
<p>Our main tool will be the package <code>rvest</code>, <span class="citation" data-cites="rvest">(Wickham <a href="#ref-rvest">2016</a>)</span> by Hadley Wickham. This is an excellent package for obtaining content from a specific webpage given its URL. However, <code>rvest</code> does not natively crawl, that is, it does not traverse from one webpage to the next. So our goal will be to implement this crawling mechanism. As a case study we will start with the Wikipedia page for “squirrel” and systematically visit all the other English Wikipedia pages that can be reached with no more than two clicks. Mathematically speaking, we are extracting the 2-neighborhood of the hyperlink network of Wikipedia’s “squirrel” page. We are restricting ourselves to the English Wikipedia, because, as we will see, this already gives us a massive hyperlink network and extending the crawling to other languages would increase the size of our network even further.</p>
<p><strong>A little brain teaser before reading on:</strong> What do you think how many (English) Wikipedia pages you can reach with just two clicks starting from the entry for “squirrel”?</p>
<h3 id="in-this-post-i-cover">In this post I cover</h3>
<ul>
<li>Obtaining content of a website (using <code>rvest</code>, <span class="citation" data-cites="rvest">(Wickham <a href="#ref-rvest">2016</a>)</span>)</li>
<li>Filtering that content for specific data (using <code>rvest</code>)</li>
<li>Teaching <code>rvest</code> how to crawl (using a <code>while loop</code> and <code>regular expressions</code>)</li>
<li>Making pretty graphs (using <code>igraph</code>, <span class="citation" data-cites="igraph">(Csardi and Nepusz <a href="#ref-igraph">2006</a>)</span>, in part 2 of this series)</li>
</ul>
<h3 id="general-strategy">General strategy</h3>
<p>Our general strategy will be</p>
<ul>
<li>Start with an initial list of urls (we will talk about how to get them later),</li>
<li>Visit each url, extract the name of the topic of the page and all the links to other pages,</li>
<li>Add the new hyperlinks to our list of urls we want to visit,</li>
<li>Keep going until we have exhausted our list or rather, until we have crawled the entire 2-neighborhood of the squirrel page.</li>
</ul>
<p>This works, because all Wikipedia pages have the same layout (i.e. the topic name will always be located in the same place) and the hyperlinks to other topics all have the same structure (i.e. <a href="https://en.wikipedia.org/" class="uri">https://en.wikipedia.org/</a><something-something>).</p>
<h1 id="a-general-word-of-caution-rate-limits-and-robots.txt">A general word of caution: rate limits and robots.txt</h1>
<p>The code and techniques discussed here are very much general purpose and hopefully some of you might find them helpful for your own web crawling project. It is important to remember, though, that we want to make sure we are not acting “anti-socially” with our crawling program. So when you embark on your own web crawling quest, you should keep two things in particular in mind.</p>
<ul>
<li>We do not want to swamp the website we are crawling with too many requests in short succession. Causing too much traffic to a website is very likely to get our IP address banned and prevent us from accessing the site for some time. To prevent this from happening, we will use the <code>Sys.sleep()</code> function after downloading each webpage. <code>Sys.sleep(n)</code> will pause our program for <code>n</code> seconds before continuing its execution.</li>
<li>Before rvesting any website, your should check their guidelines for crawling bots. These guidelines in particular contain directories of the website your are not allowed to crawl. They are stored in the <em>robots.txt</em> file which is located at <em>website-you-want-to-crawl.{com, org, co.uk,…}/robots.txt</em>. For example, the rules for crawling Wikipedia are found at <a href="https://en.wikipedia.org/robots.txt" class="uri">https://en.wikipedia.org/robots.txt</a>. When looking through the <em>robots.txt</em> file you should watch out in particular for any lines starting with</li>
</ul>
<blockquote>
<p>User-agent: *</p>
</blockquote>
<p>This line means: “The following rules apply to <em>all</em> bots”, so you should heed any rules set out after that line. In particular, if you encounter</p>
<blockquote>
<p>User-agent: *</p>
<p>Disallow: /</p>
</blockquote>
<p>this means that all bots are banned from accessing any pages on that website. In that case you are simply out of luck and should find a different website to crawl. Ignoring these rules is simply asking for trouble.</p>
<p>With all of this being said… finally on to the code!</p>
<h1 id="single-website">Single website</h1>
<p>Let’s take a look at how to obtain the information we want from a single webpage first. Once we know this, we can set up a loop to iterate over all the webpages we are interested in.</p>
<h3 id="what-do-we-want">What do we want?</h3>
<p>Take a look at: <a href="https://en.wikipedia.org/wiki/Squirrel" class="uri">https://en.wikipedia.org/wiki/Squirrel</a></p>
<p>From this page, we want to obtain:</p>
<ul>
<li>The url,</li>
<li>The name as specified by webpage topic,</li>
<li>Every hyperlink linking to another topic within the English Wikipedia.</li>
</ul>
<h3 id="how-do-we-get-what-we-want">How do we get, what we want?</h3>
<p>Make sure you have <code>rvest</code> and the <code>tidyverse</code> installed. If not, install them using the usual <code>install.packages()</code> command. Once you are good to go, downloading a website with <code>rvest</code> is as simple as running a single line of code.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# for general purpose data manipulation
library(tidyverse)
# for the actual crawling
library(rvest)
url &lt;-&quot;https://en.wikipedia.org/wiki/Squirrel&quot;
# big list of lists
page &lt;- read_html(url)</code></pre>
</div>
<p>So, what did we just download?</p>
<p>As the name suggests <code>read_html</code> reads the html content of a website. To get an idea of what this looks like, open <a href="https://en.wikipedia.org/wiki/Squirrel" class="uri">https://en.wikipedia.org/wiki/Squirrel</a> in your browser, right-click anywhere on the page and select “Inspect website” (if you are using Google Chrome that is; depending on your browser, the option should be similarly named). This opens up the website inspector and looking at the “Elements” header will show us the raw html for the site we are currently borwsing.</p>
<p><img src="squirrel_inspectelemet.png" /></p>
<p>This looks quite messy. How will we ever find the information we need in that??</p>
<p>Well, good thing we are not the first to try something like this. We can extract the topic name using so called <em>css-selectors</em>. In short, a <em>css-selector</em> is a pattern used to select elements in an html file. You can read up on them <a href="https://en.wikipedia.org/wiki/Cascading_Style_Sheets#Selector">here</a>. The nice thing is, you don’t even need to know how css-selectors, or css, for that matter, work, thanks to the great <a href="https://selectorgadget.com/">SelectorGadget</a> Chrome extension. Simply install the extension and follow the super-easy instructions on how to use it. All you need to do is navigate to the website you are interested in, click on the SelectorGadget icon in the top-right corner and click on the area of the website for which you want to know the css-selector. In our case, SelectorGadget tells us that the topic name is located at <code>#firstHeading</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
# This means: take the page we downloaded, look at what is located at &quot;#main h1&quot;
# and give me the text you find there.
name &lt;- html_text(html_node(page, css = &quot;#firstHeading&quot;))
name</code></pre>
<pre><code>
[1] &quot;Squirrel&quot;</code></pre>
</div>
<p>Very good! On to the hyperlinks.</p>
<p>Hyperlinks are stored in html using the <em>anchor tag</em>, the hyperlink is stored in the <em>href attribute</em> of the anchor tag. A text that appears in a browser as…</p>
<blockquote>
<p>Squirrels are members of the <a href="/wiki/Family_(biology)" title="Family (biology)">family</a> Sciuridae, a family that includes small or medium-size <a href="/wiki/Rodent" title="Rodent">rodents</a>.</p>
</blockquote>
<p>…in html actually looks like this:</p>
<blockquote>
<p>Squirrels are members of the &lt;a href=“/wiki/Family_(biology)” title=“Family (biology)”&gt;family&lt;/a&gt; Sciuridae, a family that includes small or medium-size &lt;a href=“/wiki/Rodent” title=“Rodent”&gt;rodents&lt;/a&gt;.</p>
</blockquote>
<p>An anchor tag starts with <code>&lt; a ...</code> and we can see that the anchor tags above each have two attributes. An <em>href</em> attribute which contains a hyperlink reference, i.e. the hyperlink to the website we want to direct the user to if they click on the link and a <em>title</em> attribute. We are interested in the href attribute. So we want to tell R “go and find all the anchor tags of the page and from within these anchor tags give me the content of the href attributes”.</p>
<p>Furthermore, looking at the layout of the squirrel page, we realize that it is structured into various sections. There is some advert at the top, the navigation bar to the left side and the actual article on squirrels. The only hyperlinks we are interested in are those located in the actual article. Playing around with SelectorGadget some more, we find out that the css-selector for the actual article is <code>#bodyContent</code>, so we want to restrict the hyperlink extraction to that bit of the page:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
links_body &lt;- page%&gt;%html_node(css = &quot;#bodyContent&quot;)%&gt;%
  html_nodes(&quot;a&quot;)%&gt;%
  html_attr(&quot;href&quot;)
head(links_body, 10)</code></pre>
<pre><code>
 [1] &quot;#mw-head&quot;                       
 [2] &quot;#p-search&quot;                      
 [3] &quot;/wiki/Squirrel_(disambiguation)&quot;
 [4] &quot;/wiki/Eocene&quot;                   
 [5] &quot;/wiki/Precambrian&quot;              
 [6] &quot;/wiki/Cambrian&quot;                 
 [7] &quot;/wiki/Ordovician&quot;               
 [8] &quot;/wiki/Silurian&quot;                 
 [9] &quot;/wiki/Devonian&quot;                 
[10] &quot;/wiki/Carboniferous&quot;            </code></pre>
<pre class="r"><code>
length(links_body)</code></pre>
<pre><code>
[1] 885</code></pre>
</div>
<p>There are 885 outgoing links from the squirrel page! But as you might have guessed from the output above, not all of them are relevant to us and we need to prune that list before we can move on. We are only interested in links to other Wikipedia pages, which conveniently are all stored in the <em>wiki</em> directory of the Wikipedia website. That is, we can filter the list we obtained above for the keyword <em>wiki</em> and get rid of all the rest. Another thing to note here is that the hyperlinks are <em>relative paths</em>. For example, if we wanted to go to the Wikipedia page on Eocene (a geological epoch), we would have to type <em><a href="https://en.wikipedia.org/wiki/Eocene" class="uri">https://en.wikipedia.org/wiki/Eocene</a></em> into the address bar of our browser. As we can see above, however, the html we downloaded only contains the last bit of that hyperlink (<em>/wiki/Eocene</em>) which is the location of the Eocene page within the Wikipedia website. When visiting the page for squirrel and clicking on the link to Eocene, our browser knows automatically that it needs to take us to the document located at the path <em>/wiki/Eocene</em> within the website we are currently on (<em><a href="https://en.wikipedia.org/" class="uri">https://en.wikipedia.org/</a></em>). <code>rvest</code> on the other hand does not know this. Therefore, once we extracted all the links containing “wiki” from vector above, we need to add the first bit of the hyperlink back on before passing the link into <code>rvest</code>’s <code>read_html</code> function.</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
links_body &lt;- page%&gt;%html_node(css = &quot;#bodyContent&quot;)%&gt;%
  html_nodes(&quot;a&quot;)%&gt;%
  html_attr(&quot;href&quot;)%&gt;%
  str_match(pattern = &quot;/wiki/[[:alnum:][:punct:]]+&quot;)%&gt;%
  subset(str_detect(string = ., pattern = &quot;:&quot;, negate = TRUE))%&gt;%
  as.character()%&gt;%
  unique()%&gt;%
  paste0(&quot;https://en.wikipedia.org&quot;, .)</code></pre>
</div>
<p>Let’s walk through this line by line: The <code>str_match</code> finds us all the links containing “/wiki/” followed by at least one more character. Next, we remove all the links containing a colon (“:”). This is to get rid of hyperlinks linking to category pages, such as “/wiki/Category:Squirrels”. That page contains an overview of various types of squirrels, but is not a Wikipedia page in its own right and hence we get rid of it. Then, we convert the entries to characters and make sure we only keep one copy of each link in case of duplicates. Finally we add “<a href="https://en.wikipedia.org" class="uri">https://en.wikipedia.org</a>” at the front of each link. The result is this</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
head(links_body, 10)</code></pre>
<pre><code>
 [1] &quot;https://en.wikipedia.org/wiki/Squirrel_(disambiguation)&quot;
 [2] &quot;https://en.wikipedia.org/wiki/Eocene&quot;                   
 [3] &quot;https://en.wikipedia.org/wiki/Precambrian&quot;              
 [4] &quot;https://en.wikipedia.org/wiki/Cambrian&quot;                 
 [5] &quot;https://en.wikipedia.org/wiki/Ordovician&quot;               
 [6] &quot;https://en.wikipedia.org/wiki/Silurian&quot;                 
 [7] &quot;https://en.wikipedia.org/wiki/Devonian&quot;                 
 [8] &quot;https://en.wikipedia.org/wiki/Carboniferous&quot;            
 [9] &quot;https://en.wikipedia.org/wiki/Permian&quot;                  
[10] &quot;https://en.wikipedia.org/wiki/Triassic&quot;                 </code></pre>
<pre class="r"><code>
length(links_body)</code></pre>
<pre><code>
[1] 512</code></pre>
</div>
<p>Nice! We can iteratively pass each of those into <code>read_html</code> and out of the 885 initial links only 512 remain.</p>
<p>To keep track of our crawling progress, we introduce a few helper objects. We will use</p>
<ul>
<li><code>to_be_checked_urls</code> to keep track of the urls we have yet to visit. We will initialize this vector with <code>links_body</code>.</li>
<li><code>checked_urls</code> to keep track of the urls we have already visited. Since we don’t want to visit any page twice, each time we download a page and extract its hyperlinks, we will first check those links against the entries in <code>checked_urls</code> before adding them to <code>to_be_checked_urls</code>. Without this we would quickly run into infinite loops. We initialize this with “<a href="https://en.wikipedia.org/wiki/Squirrel" class="uri">https://en.wikipedia.org/wiki/Squirrel</a>”.</li>
<li><code>error_urls</code> is just a precaution to keep track of any urls that we might not be able to download. Usually this only happens if there is some unforeseen bug in our code or if we disrespected some of the rules set out by the website we are crawling and got ourselves banned.</li>
<li><code>url_dictionary</code> a two-column data frame storing the topic and the corresponding url, i.e. the first row would be “squirrel” and “<a href="https://en.wikipedia.org/wiki/Squirrel" class="uri">https://en.wikipedia.org/wiki/Squirrel</a>”. This is the node-list of our network and we use it to keep track of the actual topics behind each url. This is also very useful if different hyperlinks resolve to the same page. For example, “wiki/Adjective”, “wiki/Adjectives”, “wiki/Predicate_adjective” or “wiki/Adjectival_form” all link to the page for adjective.</li>
<li><code>edges</code> is the data frame containing the actual edge information of our network. It has four columns: <code>From</code> is the name of the topic of the outgoing edge and <code>from_url</code> its url. <code>to_url</code> is the url <code>From</code> is linking to and <code>To</code> is the topic behind that url.</li>
</ul>
<p>As code this looks like this:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
to_be_checked_urls &lt;- links_body
checked_urls &lt;- url
error_urls &lt;- character()

url_dictionary &lt;- data.frame(name = name, url = url)
edges &lt;- data.frame(From = name, from_url = url, to_url = links_body, To = NA)</code></pre>
</div>
<p>And that’s it! We are ready to start crawling. If we were crawling any smaller website and actually wanted to enumerate all of the hyperlinks on it, we would use a while loop like this</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
while (length(to_be_checked_urls) &gt; 0) {
  
  # Webcrawling and updating of lists and dfs
  
}</code></pre>
</div>
<p>However, since Wikipedia is a gigantic website and we are only interested in the neighborhood of squirrel, we can use a for-loop instead and run it over all the entries in <code>links_body</code>:</p>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
for (i in 1:length(links_body)) {
  
  # read the next url to be checked
  url &lt;- to_be_checked_urls[1]
  # if we already checked that url, get rid of it
  if(sum(checked_urls == url) &gt; 0){
    to_be_checked_urls &lt;- to_be_checked_urls[-1]
    print(paste(&quot;ALREADY CHECKED&quot;, url))
  }else{
    
    # Try to download the url, if it fails for some reason report an error
    tryCatch({page &lt;- read_html(url)}, 
             error = function(e) {print(paste(&quot;URL ERROR FOR&quot;, url)); 
               error_urls &lt;- c(error_urls, url);
               to_be_checked_urls &lt;- to_be_checked_urls[-1]})
    
    # actual scraping
    name &lt;- html_text(html_node(page, css = &quot;#firstHeading&quot;))
    url_dictionary &lt;- rbind(url_dictionary,
                            data.frame(name = name,
                                       url = url))
    
    links_body &lt;- page%&gt;%html_node(css = &quot;#bodyContent&quot;)%&gt;%
      html_nodes(&quot;a&quot;)%&gt;%
      html_attr(&quot;href&quot;)%&gt;%
      str_match(pattern = &quot;/wiki/[[:alnum:][:punct:]]+&quot;)%&gt;%
      subset(str_detect(string = ., pattern = &quot;:&quot;, negate = TRUE))%&gt;%
      as.character()%&gt;%
      unique()%&gt;%
      paste0(&quot;https://en.wikipedia.org&quot;, .)
    
    # add new links to edge list
    if(length(links_body) &gt; 0){
      edges &lt;- rbind(edges,
                     data.frame(From = name,
                                from_url = url, 
                                to_url = links_body,
                                To = NA))
    }
    
    # add the newly found urls to to_be_checked_urls and update checked_urls
    #setdiff also removes duplicates: compare found urls in links_body to
    # those urls we already checked (checked_urls) and those we already know
    # to check (to_be_checked_urls)
    new_urls &lt;- setdiff(links_body, c(checked_urls, to_be_checked_urls))
    to_be_checked_urls &lt;- c(to_be_checked_urls[-1], new_urls)
    checked_urls &lt;- c(checked_urls, url)
    
    # Don&#39;t forget to pause after each download!
    Sys.sleep(1)
    
  }
}</code></pre>
</div>
<p>Note that so far the <code>To</code> column of our <code>edges</code> data frame, which is supposed to contain the topic behind <code>to_url</code>, is still set to <code>NA</code> for all rows. This is where our <code>url_dictionary</code> comes into play. Once we finished crawling all the pages we want, we can simply do a <code>dplyr::left_join(url_dictionary, by = c(&quot;to_url&quot; = &quot;url&quot;))</code> to populate that column. The exact details and other post-processing steps follow in the next blog post.</p>
<p>So, ready to find out a rough estimate for the brain teaser question from the beginning? The following code snippet gives a slight overestimation of the number of pages we can reach with two clicks from squirrel, because as mentioned above, different urls might direct to the same page. Yet, it actually already gives a pretty good estimate.</p>
<div class="layout-chunk" data-layout="l-body">

</div>
<div class="layout-chunk" data-layout="l-body">
<pre class="r"><code>
length(to_be_checked_urls)</code></pre>
<pre><code>
[1] 26625</code></pre>
</div>
<p>26625! That’s a lot! How close was your initial estimate? The actual number turns out to be slightly smaller, 22524, to be precise, but that is still a lot of pages we can reach with just two clicks. To finish the scraping, change <code>length(links_body)</code> in the loop above to <code>26625</code> and run the loop again. This will take quite a long time (at least 26625 seconds which is just shy of 7.5 hours), so when I did this project I simply left my computer running over night. There is a good chance you might get away with a shorter break between downloading pages, but I am not sure at which point Wikipedia might decide that enough is enough and block your IP. If you don’t want to wait for 7.5 hours (not that anyone could blame you), feel free to check out the corresponding <a href="https://github.com/stefan-stein/Wikipedia_webcrawling">GitHub repo</a> which contains the scraped link structure as well as the code for some basic network analysis that I will be going over in the next post. In that post we will analyse the actual link structure of the data we scraped and find some explanation as to why this number of hyperlinks is so impressive. I’d love to see you back here again for that!</p>
<div id="refs" class="references">
<div id="ref-igraph">
<p>Csardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” <em>InterJournal</em> Complex Systems: 1695. <a href="http://igraph.org">http://igraph.org</a>.</p>
</div>
<div id="ref-rvest">
<p>Wickham, Hadley. 2016. <em>Rvest: Easily Harvest (Scrape) Web Pages</em>. <a href="https://CRAN.R-project.org/package=rvest">https://CRAN.R-project.org/package=rvest</a>.</p>
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>


<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="updates-and-corrections">Corrections</h3>
<p>If you see mistakes or want to suggest changes, please <a href="https://github.com/stefan-stein/Wikipedia_webcrawling/issues/new">create an issue</a> on the source repository.</p>
</div>
<script id="distill-bibliography" type="text/bibtex">
@Manual{rvest,
    title = {rvest: Easily Harvest (Scrape) Web Pages},
    author = {Hadley Wickham},
    year = {2016},
    note = {R package version 0.3.2},
    url = {https://CRAN.R-project.org/package=rvest},
  }

@Article{igraph,
    title = {The igraph software package for complex network research},
    author = {Gabor Csardi and Tamas Nepusz},
    journal = {InterJournal},
    volume = {Complex Systems},
    pages = {1695},
    year = {2006},
    url = {http://igraph.org},
  }
</script>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
